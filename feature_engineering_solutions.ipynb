{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "# Lab 7 - Textual Data Analytics\n",
        "Complete the code with TODO tag.\n",
        "## 1. Feature Engineering\n",
        "In this exercise we will understand the functioning of TF/IDF ranking. Implement the feature engineering and its application, based on the code framework provided below.\n",
        "\n",
        "First we use textual data from Twitter."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "pycharm": {
          "is_executing": false
        }
      },
      "outputs": [
        {
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m\u003cipython-input-1-59559f4a1426\u003e\u001b[0m in \u001b[0;36m\u003cmodule\u003e\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----\u003e 3\u001b[1;33m \u001b[0mdata\u001b[0m \u001b[1;33m\u003d\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\u0027elonmusk_tweets.csv\u0027\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    684\u001b[0m     )\n\u001b[0;32m    685\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--\u003e 686\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    687\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    688\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    450\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    451\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--\u003e 452\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m\u003d\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    453\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    454\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    934\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m\u003d\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    935\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--\u003e 936\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    937\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    938\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1166\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m\u003d\u001b[0m\u001b[1;34m\"c\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1167\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m\u003d\u003d\u001b[0m \u001b[1;34m\"c\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-\u003e 1168\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m\u003d\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1169\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1170\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m\u003d\u003d\u001b[0m \u001b[1;34m\"python\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1996\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"usecols\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m\u003d\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1997\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-\u003e 1998\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m\u003d\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1999\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m\u003d\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2000\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
            "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
            "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: \u0027elonmusk_tweets.csv\u0027"
          ],
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: \u0027elonmusk_tweets.csv\u0027",
          "output_type": "error"
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "data \u003d pd.read_csv(\u0027elonmusk_tweets.csv\u0027)\n",
        "print(len(data))\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "### 1.1. Text Normalization\n",
        "Now we need to normalize text by stemming, tokenizing, and removing stopwords."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     /Users/adamnguyen/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     /Users/adamnguyen/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "from __future__ import print_function, division\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "import nltk\n",
        "nltk.download(\u0027punkt\u0027)\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "import math\n",
        "from collections import Counter\n",
        "nltk.download(\u0027stopwords\u0027)\n",
        "import pprint \n",
        "pp \u003d pprint.PrettyPrinter(indent\u003d4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[\u0027band\u0027, \u0027so\u0027, \u0027the\u0027, \u0027robot\u0027, \u0027spare\u0027, \u0027human\u0027, \u0027httpstcov7jujqwfcv\u0027]"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def normalize(document):\n",
        "    # TODO: remove punctuation\n",
        "    text \u003d \"\".join([ch for ch in document if ch not in string.punctuation])\n",
        "    \n",
        "    # TODO: tokenize text\n",
        "    tokens \u003d nltk.word_tokenize(text)\n",
        "    \n",
        "    # TODO: Stemming\n",
        "    stemmer \u003d PorterStemmer()\n",
        "    ret \u003d \" \".join([stemmer.stem(word.lower()) for word in tokens])\n",
        "    return ret\n",
        "\n",
        "original_documents \u003d [x.strip() for x in data[\u0027text\u0027]] \n",
        "documents \u003d [normalize(d).split() for d in original_documents]\n",
        "documents[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "As you can see that the normalization is still not perfect. Please feel free to improve upon (OPTIONAL), e.g. https://marcobonzanini.com/2015/03/09/mining-twitter-data-with-python-part-2/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "### 1.2. Implement TF-IDF\n",
        "Now you need to implement TF-IDF, including creating the vocabulary, computing term frequency, and normalizing by tf-idf weights."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "pycharm": {
          "is_executing": false
        }
      },
      "outputs": [
        {
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m\u003cipython-input-2-e977353f66e2\u003e\u001b[0m in \u001b[0;36m\u003cmodule\u003e\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Flatten all the documents\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----\u003e 2\u001b[1;33m \u001b[0mflat_list\u001b[0m \u001b[1;33m\u003d\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdocuments\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# TODO: remove stop words from the vocabulary\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mwords\u001b[0m \u001b[1;33m\u003d\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mflat_list\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstopwords\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\u0027english\u0027\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mNameError\u001b[0m: name \u0027documents\u0027 is not defined"
          ],
          "ename": "NameError",
          "evalue": "name \u0027documents\u0027 is not defined",
          "output_type": "error"
        }
      ],
      "source": [
        "# Flatten all the documents\n",
        "flat_list \u003d [word for doc in documents for word in doc]\n",
        "\n",
        "# TODO: remove stop words from the vocabulary\n",
        "words \u003d [word for word in flat_list if word not in stopwords.words(\u0027english\u0027)]\n",
        "\n",
        "# TODO: we take the 500 most common words only\n",
        "counts \u003d Counter(words)\n",
        "vocabulary \u003d counts.most_common(500)\n",
        "print([x for x in vocabulary if x[0] \u003d\u003d \u0027tesla\u0027])\n",
        "vocabulary \u003d [x[0] for x in vocabulary]\n",
        "assert len(vocabulary) \u003d\u003d 500\n",
        "\n",
        "# vocabulary.sort()\n",
        "vocabulary[:5]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(array([\u0027tesla\u0027, \u0027exactli\u0027], dtype\u003d\u0027\u003cU17\u0027), array([1, 1]))"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def tf(vocabulary, documents):\n",
        "    matrix \u003d [0] * len(documents)\n",
        "    for i, document in enumerate(documents):\n",
        "        counts \u003d Counter(document)\n",
        "        matrix[i] \u003d [0] * len(vocabulary)\n",
        "        for j, term in enumerate(vocabulary):\n",
        "            matrix[i][j] \u003d counts[term]\n",
        "    return matrix\n",
        "\n",
        "tf \u003d tf(vocabulary, documents)\n",
        "np.array(vocabulary)[np.where(np.array(tf[1]) \u003e 0)], np.array(tf[1])[np.where(np.array(tf[1]) \u003e 0)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[2.539126825495932,\n",
              " 3.3163095197385393,\n",
              " 3.7262581423445837,\n",
              " 3.8171115727956972,\n",
              " 3.8027562798186274]"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def idf(vocabulary, documents):\n",
        "    \"\"\"TODO: compute IDF, storing values in a dictionary\"\"\"\n",
        "    idf \u003d {}\n",
        "    num_documents \u003d len(documents)\n",
        "    for i, term in enumerate(vocabulary):\n",
        "        idf[term] \u003d math.log(num_documents / sum(term in document for document in documents), 2)\n",
        "    return idf\n",
        "\n",
        "idf \u003d idf(vocabulary, documents)\n",
        "[idf[key] for key in vocabulary[:5]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(array([\u0027tesla\u0027, \u0027exactli\u0027], dtype\u003d\u0027\u003cU17\u0027), array([3.31630952, 6.65361284]))"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def vectorize(document, vocabulary, idf):\n",
        "    vector \u003d [0]*len(vocabulary)\n",
        "    counts \u003d Counter(document)\n",
        "    for i,term in enumerate(vocabulary):\n",
        "        vector[i] \u003d idf[term] * counts[term]\n",
        "    return vector\n",
        "\n",
        "document_vectors \u003d [vectorize(s, vocabulary, idf) for s in documents]\n",
        "np.array(vocabulary)[np.where(np.array(document_vectors[1]) \u003e 0)], np.array(document_vectors[1])[np.where(np.array(document_vectors[1]) \u003e 0)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "### 1.3. Compare the results with the reference implementation of scikit-learn library."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "Now we use the scikit-learn library. As you can see that, the way we do text normalization affects the result. Feel free to further improve upon (OPTIONAL), e.g. https://stackoverflow.com/questions/36182502/add-stemming-support-to-countvectorizer-sklearn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[(\u0027http\u0027, 163.54366542841234), (\u0027https\u0027, 151.85039944652075), (\u0027rt\u0027, 112.61998731390989), (\u0027tesla\u0027, 95.96401470715628), (\u0027xe2\u0027, 88.20944486346477)]\n",
            "testla 0.3495243100660956\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import linear_kernel\n",
        "\n",
        "tfidf \u003d TfidfVectorizer(analyzer\u003d\u0027word\u0027, ngram_range\u003d(1,1), min_df \u003d 1, stop_words \u003d \u0027english\u0027, max_features\u003d500)\n",
        "\n",
        "features \u003d tfidf.fit(original_documents)\n",
        "corpus_tf_idf \u003d tfidf.transform(original_documents) \n",
        "\n",
        "sum_words \u003d corpus_tf_idf.sum(axis\u003d0)\n",
        "words_freq \u003d [(word, sum_words[0, idx]) for word, idx in tfidf.vocabulary_.items()]\n",
        "print(sorted(words_freq, key \u003d lambda x: x[1], reverse\u003dTrue)[:5])\n",
        "print(\u0027testla\u0027, corpus_tf_idf[1, features.vocabulary_[\u0027tesla\u0027]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "### 1.4.  Apply TF-IDF for information retrieval\n",
        "We can use the vector representation of documents to implement an information retrieval system. We test with the query $Q$ \u003d \"tesla nasa\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top-5 documents\n",
            "0 b\u0027@ashwin7002 @NASA @faa @AFPAA We have not ruled that out.\u0027\n",
            "1 b\u0027RT @NASA: Updated @SpaceX #Dragon #ISS rendezvous times: NASA TV coverage begins Sunday at 3:30amET: http://t.co/qrm0Dz4jPE. Grapple at  ...\u0027\n",
            "2 b\"Deeply appreciate @NASA\u0027s faith in @SpaceX. We will do whatever it takes to make NASA and the American people proud.\"\n",
            "3 b\u0027Would also like to congratulate @Boeing, fellow winner of the @NASA commercial crew program\u0027\n",
            "4 b\"@astrostephenson We\u0027re aiming for late 2015, but NASA needs to have overlapping capability to be safe. Would do the same\"\n"
          ]
        }
      ],
      "source": [
        "def cosine_similarity(v1,v2):\n",
        "    \"\"\"TODO: compute cosine similarity\"\"\"\n",
        "    sumxx, sumxy, sumyy \u003d 0, 0, 0\n",
        "    for i in range(len(v1)):\n",
        "        x \u003d v1[i]; y \u003d v2[i]\n",
        "        sumxx +\u003d x*x\n",
        "        sumyy +\u003d y*y\n",
        "        sumxy +\u003d x*y\n",
        "    if sumxy \u003d\u003d 0:\n",
        "            result \u003d 0\n",
        "    else:\n",
        "            result \u003d sumxy/math.sqrt(sumxx*sumyy)\n",
        "    return result\n",
        "\n",
        "def search_vec(query, k, vocabulary, stemmer, document_vectors, original_documents):\n",
        "    q \u003d query.split()\n",
        "    q \u003d [stemmer.stem(w) for w in q]\n",
        "    query_vector \u003d vectorize(q, vocabulary, idf)\n",
        "    \n",
        "    # TODO: rank the documents by cosine similarity\n",
        "    scores \u003d [[cosine_similarity(query_vector, document_vectors[d]), d] for d in range(len(document_vectors))]\n",
        "    scores.sort(key\u003dlambda x: -x[0])\n",
        "    \n",
        "    print(\u0027Top-{0} documents\u0027.format(k))\n",
        "    for i in range(k):\n",
        "        print(i, original_documents[scores[i][1]])\n",
        "\n",
        "query \u003d \"tesla nasa\"\n",
        "stemmer \u003d PorterStemmer()\n",
        "search_vec(query, 5, vocabulary, stemmer, document_vectors, original_documents)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "pycharm": {}
      },
      "source": [
        "We can also use the scikit-learn library to do the retrieval."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "pycharm": {}
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top-5 documents\n",
            "0 b\u0027@ashwin7002 @NASA @faa @AFPAA We have not ruled that out.\u0027\n",
            "1 b\"SpaceX could not do this without NASA. Can\u0027t express enough appreciation. https://t.co/uQpI60zAV7\"\n",
            "2 b\u0027@NASA launched a rocket into the northern lights http://t.co/tR2cSeMV\u0027\n",
            "3 b\u0027Whatever happens today, we could not have done it without @NASA, but errors are ours alone and me most of all.\u0027\n",
            "4 b\u0027RT @NASA: Updated @SpaceX #Dragon #ISS rendezvous times: NASA TV coverage begins Sunday at 3:30amET: http://t.co/qrm0Dz4jPE. Grapple at  ...\u0027\n"
          ]
        }
      ],
      "source": [
        "new_features \u003d tfidf.transform([query])\n",
        "\n",
        "cosine_similarities \u003d linear_kernel(new_features, corpus_tf_idf).flatten()\n",
        "related_docs_indices \u003d cosine_similarities.argsort()[::-1]\n",
        "\n",
        "topk \u003d 5\n",
        "print(\u0027Top-{0} documents\u0027.format(topk))\n",
        "for i in range(topk):\n",
        "    print(i, original_documents[related_docs_indices[i]])"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}